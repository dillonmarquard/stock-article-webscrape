{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d15984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Definitions\n",
    "class MaxRecursionError(Exception):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "class ArticleNotFoundError(Exception):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "class AccessDeniedError(Exception):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Client for webscraping article data\n",
    "class APIClient:\n",
    "    def __str__(self):\n",
    "        return f'Chrome Webscraper {self.driver}'\n",
    "    \n",
    "    def __init__(self):\n",
    "        # webdriver setup\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.517 Safari/537.36'\n",
    "        chrome_options.add_argument('user-agent={0}'.format(user_agent))\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=chrome_options,service_args=[\"--verbose\"])\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "        # recursion setup for auto-retry\n",
    "        self.MAX_DEPTH = 3\n",
    "\n",
    "        # blacklisted link patterns ie: yahoo finance\n",
    "        self.link_blacklist = ['finance.yahoo','sec.gov','money.cnn','markets.businessinsider.com','google.com']\n",
    "\n",
    "    ##################################\n",
    "    ##################################\n",
    "    # API\n",
    "    def get_stock_data(self,tag:str,date:str,num_links:int=25):\n",
    "        res = []\n",
    "        tmplinks = self.get_google_links(tag,date,num_links)\n",
    "        for link in tmplinks:\n",
    "            try:\n",
    "                tmpdata = self.get_article_data(link)\n",
    "                res += [tmpdata]\n",
    "            except:\n",
    "                pass\n",
    "        return res\n",
    "\n",
    "    def get_google_links(self,tag:str,date:str,num_links:int=25,recursion_depth=1):\n",
    "        # Extract links from google search\n",
    "        # Input: tag, date, num_links\n",
    "        # Output: list of URLs\n",
    "        url = \"https://www.google.com/search?q={}+news+on%3A{}&num={}\".format(tag,date,num_links)\n",
    "        res = self.get_html_from_url(url)\n",
    "        try:\n",
    "            res = self.get_links(res)\n",
    "        except:\n",
    "            if recursion_depth <= self.MAX_DEPTH: # depth check for successive retry\n",
    "                time.sleep(1) # prevent api throttling\n",
    "                res = self.get_google_links(tag,date,recursion_depth=recursion_depth+1)\n",
    "            else:\n",
    "                raise MaxRecursionError\n",
    "        return res\n",
    "    \n",
    "    def get_article_data(self,url:str,recursion_depth=1):\n",
    "        # Extract info from article\n",
    "        # Input: URL\n",
    "        # Output: article data json\n",
    "        try:\n",
    "            html = self.get_html_from_url(url)\n",
    "            data = {}\n",
    "            data['link'] = url # self identifier\n",
    "            data['title'] = self.get_title(html)\n",
    "            if re.findall('404',data['title']) != [] or re.findall('Not Found',data['title']) != [] or re.findall('Error',data['title']) != [] or re.findall('Page not found',data['title']) != []:\n",
    "                # print('Article not found.',url)\n",
    "                raise ArticleNotFoundError\n",
    "            elif data['title'] == 'Access to this page has been denied.':\n",
    "                # print('Access Denied.',url)\n",
    "                raise AccessDeniedError\n",
    "            data['text'] = self.get_text(html) # text data\n",
    "            # try:\n",
    "            #     data['metadata'] = self.get_metadata(html) # metadata (author,created_at,transcript)\n",
    "            # except:\n",
    "            #     data['metadata'] = {}\n",
    "            #     print('No metadata found.')\n",
    "        except Exception as e:\n",
    "            if recursion_depth <= self.MAX_DEPTH: # depth check for successive retry\n",
    "                time.sleep(1) # prevent api throttling\n",
    "                data = self.get_article_data(url,recursion_depth=recursion_depth+1)\n",
    "            else:\n",
    "                raise MaxRecursionError\n",
    "        return data\n",
    "    ##################################\n",
    "    ##################################\n",
    "\n",
    "    # Webdriver\n",
    "    def get_html_from_url(self,url:str):\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    # HTML Processing Helper functions\n",
    "    def get_text(self,html:str):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text = ''\n",
    "        for tmp in [tmp.text for tmp in soup.find_all(['a','p','h','h1','h2'])]:\n",
    "            if len(tmp) > 25 and re.findall('Click here',tmp) == [] and re.findall('This Simple Trick',tmp) == []:\n",
    "                text += tmp + ' '\n",
    "        text = re.sub('\\s+',' ',text.replace('\\n',' ').replace('\\xa0',' ').replace('\\'','’').replace('   ',' ').strip())\n",
    "        return text\n",
    "\n",
    "    def get_title(self,html:str):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        res = soup.find_all('title')[0].text\n",
    "        return res\n",
    "\n",
    "    # def get_metadata(self,html:str): \n",
    "    #     # Probably abandon metadata extraction because of lack of consistency between sites\n",
    "    #     soup = BeautifulSoup(html, 'html.parser')\n",
    "    #     metadata = {}\n",
    "    #     metadata['title'] = soup.find_all('title')[0].text\n",
    "    #     try:\n",
    "    #         jdata = json.loads(soup.find_all('script',type='application/ld+json')[0].text.replace('\\t','').replace('\\n','').replace('\\\\','').replace('\\\"{','{').replace('}\\\"','}'))[0]\n",
    "    #     except:\n",
    "    #         try:\n",
    "    #             jdata = json.loads(soup.find_all('script',type='application/ld+json')[0].text.replace('\\t','').replace('\\n','').replace('\\\\','').replace('\\\"{','{').replace('}\\\"','}'))\n",
    "    #         except:\n",
    "    #             try:\n",
    "    #                 jdata = json.loads(soup.find_all('script',type='application/ld+json')[0].text)[0]\n",
    "    #             except:\n",
    "    #                 jdata = json.loads(soup.find_all('script',type='application/ld+json')[0].text.replace('\\n','').replace('   ','').replace('\\\\','').replace('\\t',''))\n",
    "    #     try:\n",
    "    #         metadata['author'] = jdata['author'][0]['name']\n",
    "    #     except:\n",
    "    #         try:\n",
    "    #             metadata['author'] = jdata['author']['name']\n",
    "    #         except:\n",
    "    #             try:\n",
    "    #                 metadata['author'] = jdata['liveBlogUpdate'][0]['author']['name']\n",
    "    #             except:\n",
    "    #                 try:\n",
    "    #                     jdata['@graph'][0]['author']['name']\n",
    "    #                 except:\n",
    "    #                     try:\n",
    "    #                         metadata['author'] = jdata['author'][0]\n",
    "    #                     except:\n",
    "    #                         print('No author found.')\n",
    "    #     try:  \n",
    "    #         metadata['published_at'] = jdata['datePublished']\n",
    "    #     except:\n",
    "    #         print('No publish date found.')\n",
    "    #     try:\n",
    "    #         metadata['description'] = jdata['description']\n",
    "    #     except:\n",
    "    #         print('No description found.')\n",
    "    #     try:\n",
    "    #         # bloomberg article had one\n",
    "    #         metadata['transcript'] = re.sub('/  +/g',' ',re.sub('/  +/g',' ',jdata['video']['transcript']).replace('\\n',' ').replace('  ','').replace('\\'','’'))\n",
    "    #     except Exception as e:\n",
    "    #         print('No transcript found.')\n",
    "    #     return metadata\n",
    "\n",
    "    #####\n",
    "    # JSON metadata extraction types\n",
    "    \n",
    "    #####\n",
    "    def get_links(self,html:str):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = []\n",
    "        for x in soup.find_all('a'):\n",
    "            try:\n",
    "                tmp = x['href'].replace('/url?esrc=s&q=&rct=j&sa=U&url=','')\n",
    "                if 'https://' == tmp[0:8] and min([re.findall(pattern,tmp) == [] for pattern in self.link_blacklist]): # check for blacklisted pattern\n",
    "                    links += [tmp]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return links # ignore first eight links from google\n",
    "\n",
    "    def __del__(self):\n",
    "        self.driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251aa268",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = APIClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = api.get_stock_data('tsla','2023-01-03',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "251455e4",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://www.bloomberg.com/news/articles/2023-02-17/summers-says-too-soon-to-call-for-march-50-basis-point-fed-hike?srnd=premium\"\n",
    "#url = \"https://www.bloomberg.com/news/articles/2023-02-18/cars-tires-textile-factories-have-shut-in-crisis-hit-pakistan?srnd=industries-v2\"\n",
    "\n",
    "testlinks = api.get_google_links('tsla','2022-02-01',25)\n",
    "testlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = api.get_article_data(testlinks[4])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = api.get_html_from_url(testlinks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22635367",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "text = ''\n",
    "for tmp in [tmp.text for tmp in soup.find_all(['a','p','h','h1','h2'])]:\n",
    "    if len(tmp) > 25 and re.findall('Click here',tmp) == [] and re.findall('This Simple Trick',tmp) == []:\n",
    "        text += tmp + ' '\n",
    "text = re.sub('\\s+',' ',text.replace('\\n',' ').replace('\\xa0',' ').replace('\\'','’').replace('   ',' ').strip())\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b57d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "08d2074b7bb278368f8ab24694ebbfe757e89490b0a59eca29ce192a5b475499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
